[2024-08-03 01:42:59,743] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-08-03 01:43:09,296] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-08-03 01:43:09,327] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.2
[93m [WARNING] [0m using untested triton version (2.2.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.2
[93m [WARNING] [0m using untested triton version (2.2.0), only 1.0.0 is known to be compatible
[2024-08-03 01:43:10,958] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-08-03 01:43:10,958] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-08-03 01:43:10,958] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
Max steps: 15
Max steps: 15
Size of the train set: 701085. Size of the validation set: 300466
A sample of train dataset: {'content': '<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n\n"Hi, I\'m new to assembly level programming and I need some help. I have defined a byte in my alp code like this:\n\n`count db 0`\n\nIn another part of my program, I want to fetch the value of `count` into the `ah` register and then add 1 to it. Here\'s what I have so far:\n\n```\nmov ah,count\nadd ah,1\n```\n\nNow, my question is, how can I update the memory location recognized by `count` with the incremented value in the `ah` register? I need to be able to use this updated value as a counter for looping or for any other purposes. Can someone please guide me on how to achieve this?"<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\nTo update the memory location recognized by `count` with the incremented value in the `ah` register, you need to make a few changes to your code. \n\nFirst, `mov ah,count` won\'t work because in Nasm syntax, it tries to move the address of `count` into `ah`, which won\'t fit. Instead, you need to use `[contents]` to access the value stored at the memory location pointed by `count`. \n\nHere\'s the modified code:\n\n```\nmov ah, [count]  ; Load the value stored at the memory location pointed by count into ah\ninc ah           ; Increment the value in ah by 1\nmov [count], ah  ; Store the updated value from ah back into the memory location pointed by count\n```\n\nNow, the value stored in `count` will be updated with the incremented value from `ah`. You can use this updated value as a counter for looping or for any other purposes.\n\nAlternatively, if you want to directly increment the value at the memory location pointed by `count` without using the `ah` register, you can use the following code:\n\n```\ninc byte [count]  ; Increment the byte value at the memory location pointed by count\n```\n\nYou can also add additional logic, such as comparing the incremented value with a maximum count and jumping to a loop label if the condition is met. Here\'s an example:\n\n```\ncmp byte [count], MAXCOUNT  ; Compare the byte value at the memory location pointed by count with a maximum count\njb looptop                 ; Jump to the label "looptop" if the comparison result is less than (jb) the maximum count\n```\n\nMake sure to define `MAXCOUNT` and `looptop` appropriately before using these additional logic.\n\nRemember, working with registers is generally faster than working with memory, so it\'s recommended to use registers whenever possible. However, both approaches will achieve the desired result. I hope this helps!<|eot_id|>'}
Size of the train set: 701085. Size of the validation set: 300466
A sample of train dataset: {'content': '<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n\n"Hi, I\'m new to assembly level programming and I need some help. I have defined a byte in my alp code like this:\n\n`count db 0`\n\nIn another part of my program, I want to fetch the value of `count` into the `ah` register and then add 1 to it. Here\'s what I have so far:\n\n```\nmov ah,count\nadd ah,1\n```\n\nNow, my question is, how can I update the memory location recognized by `count` with the incremented value in the `ah` register? I need to be able to use this updated value as a counter for looping or for any other purposes. Can someone please guide me on how to achieve this?"<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\nTo update the memory location recognized by `count` with the incremented value in the `ah` register, you need to make a few changes to your code. \n\nFirst, `mov ah,count` won\'t work because in Nasm syntax, it tries to move the address of `count` into `ah`, which won\'t fit. Instead, you need to use `[contents]` to access the value stored at the memory location pointed by `count`. \n\nHere\'s the modified code:\n\n```\nmov ah, [count]  ; Load the value stored at the memory location pointed by count into ah\ninc ah           ; Increment the value in ah by 1\nmov [count], ah  ; Store the updated value from ah back into the memory location pointed by count\n```\n\nNow, the value stored in `count` will be updated with the incremented value from `ah`. You can use this updated value as a counter for looping or for any other purposes.\n\nAlternatively, if you want to directly increment the value at the memory location pointed by `count` without using the `ah` register, you can use the following code:\n\n```\ninc byte [count]  ; Increment the byte value at the memory location pointed by count\n```\n\nYou can also add additional logic, such as comparing the incremented value with a maximum count and jumping to a loop label if the condition is met. Here\'s an example:\n\n```\ncmp byte [count], MAXCOUNT  ; Compare the byte value at the memory location pointed by count with a maximum count\njb looptop                 ; Jump to the label "looptop" if the comparison result is less than (jb) the maximum count\n```\n\nMake sure to define `MAXCOUNT` and `looptop` appropriately before using these additional logic.\n\nRemember, working with registers is generally faster than working with memory, so it\'s recommended to use registers whenever possible. However, both approaches will achieve the desired result. I hope this helps!<|eot_id|>'}
Adam Optimizer #0 is created with AVX2 arithmetic capability.
Config: alpha=0.000100, betas=(0.900000, 0.999000), weight_decay=0.010000, adam_w=1
[2024-08-03 01:45:53,634] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.14.5+ffd0a0e3, git-hash=ffd0a0e3, git-branch=master
[2024-08-03 01:45:54,380] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2024-08-03 01:45:54,383] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2024-08-03 01:45:54,383] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2024-08-03 01:45:54,411] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam
[2024-08-03 01:45:54,411] [INFO] [utils.py:59:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>
[2024-08-03 01:45:54,411] [INFO] [logging.py:96:log_dist] [Rank 0] Creating fp16 ZeRO stage 3 optimizer, MiCS is enabled False, Hierarchical params gather False
[2024-08-03 01:45:54,411] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 3 optimizer
[2024-08-03 01:45:54,503] [INFO] [utils.py:781:see_memory_usage] Stage 3 initialize beginning
[2024-08-03 01:45:54,505] [INFO] [utils.py:782:see_memory_usage] MA 5.35 GB         Max_MA 5.35 GB         CA 5.35 GB         Max_CA 5 GB 
[2024-08-03 01:45:54,505] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 18.04 GB, percent = 3.6%
[2024-08-03 01:45:54,512] [INFO] [stage3.py:130:__init__] Reduce bucket size 500,000,000
[2024-08-03 01:45:54,512] [INFO] [stage3.py:131:__init__] Prefetch bucket size 50,000,000
[2024-08-03 01:45:54,593] [INFO] [utils.py:781:see_memory_usage] DeepSpeedZeRoOffload initialize [begin]
[2024-08-03 01:45:54,593] [INFO] [utils.py:782:see_memory_usage] MA 5.35 GB         Max_MA 5.35 GB         CA 5.35 GB         Max_CA 5 GB 
[2024-08-03 01:45:54,593] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 18.04 GB, percent = 3.6%
Parameter Offload: Total persistent parameters: 10227712 in 417 params
[2024-08-03 01:45:55,729] [INFO] [utils.py:781:see_memory_usage] DeepSpeedZeRoOffload initialize [end]
[2024-08-03 01:45:55,730] [INFO] [utils.py:782:see_memory_usage] MA 0.1 GB         Max_MA 5.35 GB         CA 5.35 GB         Max_CA 5 GB 
[2024-08-03 01:45:55,730] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 22.45 GB, percent = 4.5%
[2024-08-03 01:45:55,829] [INFO] [utils.py:781:see_memory_usage] Before creating fp16 partitions
[2024-08-03 01:45:55,830] [INFO] [utils.py:782:see_memory_usage] MA 0.1 GB         Max_MA 0.1 GB         CA 5.35 GB         Max_CA 5 GB 
[2024-08-03 01:45:55,830] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 22.45 GB, percent = 4.5%
[2024-08-03 01:45:57,590] [INFO] [utils.py:781:see_memory_usage] After creating fp16 partitions: 1
[2024-08-03 01:45:57,591] [INFO] [utils.py:782:see_memory_usage] MA 0.1 GB         Max_MA 0.1 GB         CA 5.35 GB         Max_CA 5 GB 
[2024-08-03 01:45:57,591] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 22.59 GB, percent = 4.5%
[2024-08-03 01:45:57,695] [INFO] [utils.py:781:see_memory_usage] Before creating fp32 partitions
[2024-08-03 01:45:57,695] [INFO] [utils.py:782:see_memory_usage] MA 0.1 GB         Max_MA 0.1 GB         CA 5.35 GB         Max_CA 5 GB 
[2024-08-03 01:45:57,695] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 22.59 GB, percent = 4.5%
[2024-08-03 01:45:57,821] [INFO] [utils.py:781:see_memory_usage] After creating fp32 partitions
[2024-08-03 01:45:57,822] [INFO] [utils.py:782:see_memory_usage] MA 0.1 GB         Max_MA 0.1 GB         CA 5.35 GB         Max_CA 5 GB 
[2024-08-03 01:45:57,822] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 22.65 GB, percent = 4.5%
[2024-08-03 01:45:57,934] [INFO] [utils.py:781:see_memory_usage] Before initializing optimizer states
[2024-08-03 01:45:57,935] [INFO] [utils.py:782:see_memory_usage] MA 0.1 GB         Max_MA 0.1 GB         CA 5.35 GB         Max_CA 5 GB 
[2024-08-03 01:45:57,935] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 22.69 GB, percent = 4.5%
[2024-08-03 01:45:58,064] [INFO] [utils.py:781:see_memory_usage] After initializing optimizer states
[2024-08-03 01:45:58,065] [INFO] [utils.py:782:see_memory_usage] MA 0.1 GB         Max_MA 0.1 GB         CA 5.35 GB         Max_CA 5 GB 
[2024-08-03 01:45:58,065] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 22.73 GB, percent = 4.5%
[2024-08-03 01:45:58,065] [INFO] [stage3.py:485:_setup_for_real_optimizer] optimizer state initialized
[2024-08-03 01:45:58,416] [INFO] [utils.py:781:see_memory_usage] After initializing ZeRO optimizer
[2024-08-03 01:45:58,416] [INFO] [utils.py:782:see_memory_usage] MA 1.03 GB         Max_MA 1.04 GB         CA 5.35 GB         Max_CA 5 GB 
[2024-08-03 01:45:58,417] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 22.8 GB, percent = 4.5%
[2024-08-03 01:45:58,417] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedZeroOptimizer_Stage3
[2024-08-03 01:45:58,417] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2024-08-03 01:45:58,417] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2024-08-03 01:45:58,417] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0001], mom=[(0.9, 0.999)]
[2024-08-03 01:45:58,421] [INFO] [config.py:997:print] DeepSpeedEngine configuration:
[2024-08-03 01:45:58,421] [INFO] [config.py:1001:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2024-08-03 01:45:58,421] [INFO] [config.py:1001:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2024-08-03 01:45:58,421] [INFO] [config.py:1001:print]   amp_enabled .................. False
[2024-08-03 01:45:58,421] [INFO] [config.py:1001:print]   amp_params ................... False
[2024-08-03 01:45:58,421] [INFO] [config.py:1001:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2024-08-03 01:45:58,421] [INFO] [config.py:1001:print]   bfloat16_enabled ............. True
[2024-08-03 01:45:58,421] [INFO] [config.py:1001:print]   bfloat16_immediate_grad_update  False
[2024-08-03 01:45:58,421] [INFO] [config.py:1001:print]   checkpoint_parallel_write_pipeline  False
[2024-08-03 01:45:58,421] [INFO] [config.py:1001:print]   checkpoint_tag_validation_enabled  True
[2024-08-03 01:45:58,421] [INFO] [config.py:1001:print]   checkpoint_tag_validation_fail  False
[2024-08-03 01:45:58,421] [INFO] [config.py:1001:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x1485901662f0>
[2024-08-03 01:45:58,421] [INFO] [config.py:1001:print]   communication_data_type ...... None
[2024-08-03 01:45:58,421] [INFO] [config.py:1001:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2024-08-03 01:45:58,421] [INFO] [config.py:1001:print]   curriculum_enabled_legacy .... False
[2024-08-03 01:45:58,422] [INFO] [config.py:1001:print]   curriculum_params_legacy ..... False
[2024-08-03 01:45:58,422] [INFO] [config.py:1001:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2024-08-03 01:45:58,422] [INFO] [config.py:1001:print]   data_efficiency_enabled ...... False
[2024-08-03 01:45:58,422] [INFO] [config.py:1001:print]   dataloader_drop_last ......... False
[2024-08-03 01:45:58,422] [INFO] [config.py:1001:print]   disable_allgather ............ False
[2024-08-03 01:45:58,422] [INFO] [config.py:1001:print]   dump_state ................... False
[2024-08-03 01:45:58,422] [INFO] [config.py:1001:print]   dynamic_loss_scale_args ...... None
[2024-08-03 01:45:58,422] [INFO] [config.py:1001:print]   eigenvalue_enabled ........... False
[2024-08-03 01:45:58,422] [INFO] [config.py:1001:print]   eigenvalue_gas_boundary_resolution  1
[2024-08-03 01:45:58,422] [INFO] [config.py:1001:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2024-08-03 01:45:58,422] [INFO] [config.py:1001:print]   eigenvalue_layer_num ......... 0
[2024-08-03 01:45:58,422] [INFO] [config.py:1001:print]   eigenvalue_max_iter .......... 100
[2024-08-03 01:45:58,422] [INFO] [config.py:1001:print]   eigenvalue_stability ......... 1e-06
[2024-08-03 01:45:58,422] [INFO] [config.py:1001:print]   eigenvalue_tol ............... 0.01
[2024-08-03 01:45:58,422] [INFO] [config.py:1001:print]   eigenvalue_verbose ........... False
[2024-08-03 01:45:58,422] [INFO] [config.py:1001:print]   elasticity_enabled ........... False
[2024-08-03 01:45:58,422] [INFO] [config.py:1001:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2024-08-03 01:45:58,422] [INFO] [config.py:1001:print]   fp16_auto_cast ............... None
[2024-08-03 01:45:58,422] [INFO] [config.py:1001:print]   fp16_enabled ................. False
[2024-08-03 01:45:58,422] [INFO] [config.py:1001:print]   fp16_master_weights_and_gradients  False
[2024-08-03 01:45:58,422] [INFO] [config.py:1001:print]   global_rank .................. 0
[2024-08-03 01:45:58,422] [INFO] [config.py:1001:print]   grad_accum_dtype ............. None
[2024-08-03 01:45:58,422] [INFO] [config.py:1001:print]   gradient_accumulation_steps .. 2
[2024-08-03 01:45:58,422] [INFO] [config.py:1001:print]   gradient_clipping ............ 1.0
[2024-08-03 01:45:58,422] [INFO] [config.py:1001:print]   gradient_predivide_factor .... 1.0
[2024-08-03 01:45:58,422] [INFO] [config.py:1001:print]   graph_harvesting ............. False
[2024-08-03 01:45:58,422] [INFO] [config.py:1001:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2024-08-03 01:45:58,422] [INFO] [config.py:1001:print]   initial_dynamic_scale ........ 1
[2024-08-03 01:45:58,422] [INFO] [config.py:1001:print]   load_universal_checkpoint .... False
[2024-08-03 01:45:58,423] [INFO] [config.py:1001:print]   loss_scale ................... 1.0
[2024-08-03 01:45:58,423] [INFO] [config.py:1001:print]   memory_breakdown ............. False
[2024-08-03 01:45:58,423] [INFO] [config.py:1001:print]   mics_hierarchial_params_gather  False
[2024-08-03 01:45:58,423] [INFO] [config.py:1001:print]   mics_shard_size .............. -1
[2024-08-03 01:45:58,423] [INFO] [config.py:1001:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2024-08-03 01:45:58,423] [INFO] [config.py:1001:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2024-08-03 01:45:58,423] [INFO] [config.py:1001:print]   optimizer_legacy_fusion ...... False
[2024-08-03 01:45:58,423] [INFO] [config.py:1001:print]   optimizer_name ............... None
[2024-08-03 01:45:58,423] [INFO] [config.py:1001:print]   optimizer_params ............. None
[2024-08-03 01:45:58,423] [INFO] [config.py:1001:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2024-08-03 01:45:58,423] [INFO] [config.py:1001:print]   pld_enabled .................. False
[2024-08-03 01:45:58,423] [INFO] [config.py:1001:print]   pld_params ................... False
[2024-08-03 01:45:58,423] [INFO] [config.py:1001:print]   prescale_gradients ........... False
[2024-08-03 01:45:58,423] [INFO] [config.py:1001:print]   scheduler_name ............... None
[2024-08-03 01:45:58,423] [INFO] [config.py:1001:print]   scheduler_params ............. None
[2024-08-03 01:45:58,423] [INFO] [config.py:1001:print]   seq_parallel_communication_data_type  torch.float32
[2024-08-03 01:45:58,423] [INFO] [config.py:1001:print]   sparse_attention ............. None
[2024-08-03 01:45:58,423] [INFO] [config.py:1001:print]   sparse_gradients_enabled ..... False
[2024-08-03 01:45:58,423] [INFO] [config.py:1001:print]   steps_per_print .............. inf
[2024-08-03 01:45:58,423] [INFO] [config.py:1001:print]   timers_config ................ enabled=True synchronized=True
[2024-08-03 01:45:58,423] [INFO] [config.py:1001:print]   train_batch_size ............. 16
[2024-08-03 01:45:58,423] [INFO] [config.py:1001:print]   train_micro_batch_size_per_gpu  4
[2024-08-03 01:45:58,423] [INFO] [config.py:1001:print]   use_data_before_expert_parallel_  False
[2024-08-03 01:45:58,423] [INFO] [config.py:1001:print]   use_node_local_storage ....... False
[2024-08-03 01:45:58,423] [INFO] [config.py:1001:print]   wall_clock_breakdown ......... False
[2024-08-03 01:45:58,423] [INFO] [config.py:1001:print]   weight_quantization_config ... None
[2024-08-03 01:45:58,423] [INFO] [config.py:1001:print]   world_size ................... 2
[2024-08-03 01:45:58,423] [INFO] [config.py:1001:print]   zero_allow_untested_optimizer  True
[2024-08-03 01:45:58,424] [INFO] [config.py:1001:print]   zero_config .................. stage=3 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='cpu', nvme_path=None, buffer_count=5, buffer_size=100,000,000, max_in_cpu=1,000,000,000, pin_memory=False) offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=None, buffer_count=4, pin_memory=False, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False, ratio=1.0) sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=True use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2024-08-03 01:45:58,424] [INFO] [config.py:1001:print]   zero_enabled ................. True
[2024-08-03 01:45:58,424] [INFO] [config.py:1001:print]   zero_force_ds_cpu_optimizer .. True
[2024-08-03 01:45:58,424] [INFO] [config.py:1001:print]   zero_optimization_stage ...... 3
[2024-08-03 01:45:58,424] [INFO] [config.py:987:print_user_config]   json = {
    "train_batch_size": 16, 
    "train_micro_batch_size_per_gpu": 4, 
    "gradient_accumulation_steps": 2, 
    "zero_optimization": {
        "stage": 3, 
        "offload_optimizer": {
            "device": "cpu", 
            "nvme_path": null
        }, 
        "offload_param": {
            "device": "cpu", 
            "nvme_path": null
        }, 
        "stage3_gather_16bit_weights_on_model_save": true
    }, 
    "gradient_clipping": 1.0, 
    "steps_per_print": inf, 
    "bf16": {
        "enabled": true
    }, 
    "fp16": {
        "enabled": false
    }, 
    "zero_allow_untested_optimizer": true
}
Completed step 1
Completed step 1
Completed step 2
Completed step 2
Completed step 3
Completed step 3
Completed step 4
Completed step 4
Completed step 5
Completed step 5
{'loss': 1.6383, 'grad_norm': 0.6257875011193782, 'learning_rate': 7.500000000000001e-05, 'epoch': 0.0}
Completed step 6
Completed step 6
Completed step 7
Completed step 7
Completed step 8Completed step 8

Completed step 9
Completed step 9
Completed step 10
Completed step 10
{'loss': 1.5059, 'grad_norm': 0.42974065971987147, 'learning_rate': 2.500000000000001e-05, 'epoch': 0.0}
Completed step 11
Completed step 11
Completed step 12
Completed step 12
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.2
[93m [WARNING] [0m using untested triton version (2.2.0), only 1.0.0 is known to be compatible
